# RAG Playground

RAG Playground is a platform to experiment with RAG (Retrieval Augmented Generation) techniques. It integrates with LangChain and Vertex AI, allowing you to compare different retrieval methods and/or LLMs on your own datasets. This helps you build, refine, and evaluate RAG-based applications.

## 📍 Overview

RAG Playground is a tool / framework that aims to solve the following high level goals:

1. Provide a unified interface to experiment with various RAG components and configurations (parsing, chunking, retrieval, answering, etc.).
2. Enable easy comparison of answers generated by various techniques for the same query.
3. Provide automated side-by-side evaluations using first-party and third-party frameworks (e.g., Vertex GenAI Rapid Evals). Includes a voting system to gather user preferences and create a "gold standard" dataset for evaluation.
4. Leverage the generated "gold standard" dataset for evaluating new RAG techniques.
5. Leverage user preferences to select the best technique for specific queries. Utilize these preferences to fine-tune LLMs using reinforcement learning with human feedback (RLHF).
6. Easily extensible toolkit extending across any RAG system, component, or evaluation method. Out-of-the-box support starting with Vertex AI Search and DIY RAG APIs.

## 🔗 Quick Links

- [📦 Features](features.md)
- [📂 Repository Structure](repository-structure.md)
- [🏗️ Architecture](architecture.md)
- [🔄 Data Processing Pipeline](data-processing-pipeline.md)
- [🖥️ Serving Layer](serving-layer.md)
- [🛠️ Setup](setup.md)
- [🏃 Getting Started](running-locally.md)
- [🚀 Deployment](deployment.md)
- [🤝 Contributing](contributing.md)
<!-- - [👏 Acknowledgments](acknowledgments.md) -->